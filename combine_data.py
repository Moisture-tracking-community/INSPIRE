"""Combine data.

This module contains functionality for reading the data contributed to the
comparison. Each contributed dataset has can be loaded with its own function.
There is also a function `read_data_pakistan` that reads data for all models in
one go.

Each data reading function gets the base directory as a pathlib Path object, and
should return an xarray dataset, with one or more variables that carry the name(s)
of the model/method or ensemble members.
"""

from pathlib import Path

import numpy as np
import xarray as xr

from Functions import grid_cell_area


def read_wam2layers(basedir, casename):
    """Read data for WAM2layers."""
    print(f"Loading wam2layers data for {casename}")
    path = basedir / casename / "results WAM2layers"
    filename_pattern = "backtrack_*T00-00.nc"

    ds = (
        xr.open_mfdataset(
            path.glob(filename_pattern),
            combine="nested",
            concat_dim="time",
        )["e_track"]
        .rename(latitude="lat", longitude="lon")
        .sum("time")
        .rename("wam2layers")
    )

    if casename == "Pakistan":
        # Convert units
        # TODO might not be necessary with latest version of data, see
        # https://github.com/WAM2layers/Moisture_tracking_intercomparison/issues/43
        return ds / grid_cell_area(ds.lat, ds.lon) * 1000
    else:
        return ds


def read_wrf_wvt(basedir, casename):
    print(
        "Skipping wrf_wvt data for {casename} as it is not available in gridded form."
    )


def read_uvigo(basedir, casename):
    """Read data from University of Vigo"""
    print(f"Loading uvigo data for {casename}")

    path = basedir / casename / "results Uvigo"
    if casename == "Scotland":
        return xr.Dataset(
            {
                "Vigo_e1_Stohl": xr.open_dataarray(path / "ERA5_Stohl_backwardreg.nc"),
                "Vigo_e2_Sodemann": xr.open_dataarray(path / "ERA5_sodemann_reg.nc"),
            }
        )
    else:
        return xr.Dataset(
            {
                "Vigo_e1_Stohl": xr.open_dataarray(path / "ERA5_SJ05_reg.nc"),
                "Vigo_e2_Sodemann": xr.open_dataarray(path / "ERA5_APA22_reg.nc"),
            }
        )


def read_utrack(basedir, casename):
    """Read data from UTRACK.

    Data is generated by Arie Staal.

    Based on the communication with Arie the
    results should be area corrected. Given the values I assumed that the area
    needs to be in km**2, however this should be checked.
    """
    print(f"Loading utrack data for {casename}")
    # TODO check units with Arie
    n_gridcells_pakistan = (71 - 67) / 0.25 * (30 - 24) / 0.25

    path = basedir / casename / "results Utrack Arie Staal/"

    if casename == "Pakistan":
        path = path / "moisture_tracking_intercomparsion"

    ensemble_members = {
        "utrack_e1": path.glob("*_mixing48h_dt025h_100p.nc"),
        "utrack_e2": path.glob("*_mixing24h_dt025h_100p.nc"),
        "utrack_e3": path.glob("*_mixing12h_dt025h_100p.nc"),
        "utrack_e4": path.glob("*_mixing24h_dt025h_1000p.nc"),
        "utrack_e5": path.glob("*_mixing24h_dt010h_100p.nc"),
    }

    ensemble = {}
    for member, files in ensemble_members.items():
        ensemble[member] = xr.open_mfdataset(
            files,
            combine="nested",
            concat_dim="time",
        )["moisture_source"].sum("time")

    if casename == "Pakistan":
        return xr.Dataset(ensemble) * n_gridcells_pakistan
    else:
        ds = xr.Dataset(ensemble)
        return ds * grid_cell_area(ds.lat, ds.lon) / 10**6


def read_ughent(basedir, casename):
    """Read data from university of Ghent.

    Generated with HAMSTER.
    """
    print(f"Loading ughent data for {casename}")
    path = (
        basedir
        / casename
        / f"results UGhent HAMSTER/{casename} simulations/bias_corrected"
    )
    filename_pattern = "bias_corrected_*_{ensemble}.nc"

    ensemble_members = {
        "ghent_e1": "ens1_sod08",
        "ghent_e2": "ens2_fas19",
        "ghent_e3": "ens3_rh20",
        "ghent_e4": "ens4_allabl",
        "ghent_e5": "ens5_rhadap80",
    }

    ensemble = {}
    for name, ens in ensemble_members.items():
        files = (path / ens).glob(filename_pattern.format(ensemble=ens[5:]))
        ensemble[name] = xr.open_mfdataset(
            files, combine="nested", concat_dim="time"
        ).sum("time")["E2P_BC"]

    return xr.Dataset(ensemble)


def read_tracmass(basedir, casename):
    """Read tracmass data.

    Data supplied by Dipanjan Dey
    Units in mm/day, so multiplied with # of event days.
    """
    if casename == "Australia":
        print("Skipping tracmass data for {casename} - not available")
        return xr.Dataset()
    elif casename == "Pakistan":
        filename = "TRACMASS_diagnostics.nc"
    elif casename == "Scotland":
        filename = "TRACMASS_evap_sources_06-08oct2023.nc"

    print(f"Loading tracmass data for {casename}")

    nrdays = 15
    path = basedir / casename / "results TRACMASS Dipanjan Dey"

    ds = xr.open_dataset(path / filename)  # Evaporative sources (and preicp?) mm/day

    # convert to -180 to 180 lon
    ds.coords["lon"] = (ds.coords["lon"] + 180) % 360 - 180
    ds = ds.sortby(ds.lon)

    # Units of data is mm/day but we want mm over whole time period
    return ds["E_TRACMASS"].rename("TRACMASS") * nrdays


def read_flexpart_tatfancheng(basedir, casename):
    """Read flexpart data from tatfancheng.

    Generated with FLEXPART-Watersip
    """
    print(f"Loading tatfancheng data for {casename}")
    path = basedir / casename / "results FLEXPART_WaterSip_TatFanCheng"

    if casename == "Pakistan":
        filename = "WaterSip_Cb_20220810-20220824_Pakistan_box.nc"
        ds = xr.open_dataset(path / filename)

        # convert to -180 to 180 lon
        ds.coords["lon"] = (ds.coords["lon"] + 180) % 360 - 180

        return ds.sortby(ds.lon).sum("time")["Cb"].rename("flexpart_tatfancheng")

    elif casename in ["Scotland", "Australia"]:
        if casename == "Scotland":
            date = "20231006-20231008"
        else:
            date = "20220222-20220228"
        ensemble_members = ["Ens1", "Ens2", "Ens3"]
        ensemble = {}
        for member in ensemble_members:
            filename = f"WaterSip_moisture_source_{casename}_{date}_{member}.nc"
            ds = xr.open_dataset(path / filename)
            # convert to -180 to 180 lon
            ds["lon"] = (ds["lon"] + 180) % 360 - 180
            ensemble[f"flexpart_tatfancheng_{member}"] = ds.sortby(ds.lon)["Cb"]

        return xr.Dataset(ensemble)


def read_flexpart_xu(basedir, casename):
    """Read flexpart data from Xu."""
    print(f"Loading xu data for {casename}")
    path = basedir / casename / "results Ru_Xu_FLEXPART"

    if casename == "Pakistan":
        filename = "e_daily.nc"
        variable = "variable"
        remap_vars = dict(latitude="lat", longitude="lon")
    elif casename == "Australia":
        filename = "aus_e_daily.nc"
        variable = "data"
        remap_vars = {}
    elif casename == "Scotland":
        filename = "scot_e_daily.nc"
        variable = "data"
        remap_vars = {}

    # Load, rename coords, select variable, and accumulate over time
    return (
        xr.open_dataset(path / filename)[variable]
        .rename(remap_vars)
        .sum("time")
        .rename("flexpart_xu")
    )


def read_lagranto_chc(basedir, casename):
    """Read lagranto CHc data."""
    print(f"Loading CHc data for {casename}")
    path = basedir / casename / "results CHc LAGRANTO"

    if casename == "Pakistan":
        nlon = 1440
        year = 2022
    elif casename == "Australia":
        nlon = 1441
        year = 2022
    elif casename == "Scotland":
        nlon = 1441
        year = 2023

    filename = f"{casename}_{year}_CHc_eventtotal_ens1.nc"
    return (
        xr.open_dataset(path / filename)
        .rename(dimx_N="lon", dimy_N="lat")["N"]
        .sum("time")
        .squeeze()
        .assign_coords(lat=np.linspace(-90, 90, 721), lon=np.linspace(-180, 180, nlon))
        .rename("lagranto_CHc")
    )


def read_flexpart_univie(basedir, casename):
    """Read data for Flexpart UniVie."""
    print(f"Loading UniVie data for {casename}")
    path = basedir / casename / "results univie FLEXPART"
    filename = f"{casename.lower()}_univie.nc"

    ds = xr.open_dataset(path / filename).sum("time")
    combined = ds["moisture_uptakes_bl"] + ds["moisture_uptakes_ft"]

    return combined.rename("flexpart_univie")


def read_2ldrm(basedir, casename):
    """Read data for 2ldrm."""
    print(f"Loading 2ldrm data for {casename}")
    path = basedir / casename / "results 2LDRM"
    filename = f"2LDRM_{casename}_case_gl.nc"

    ds = (
        xr.open_dataset(path / filename)
        .rename(latitude="lat", longitude="lon")["moisture_source"]
        .sum("time")
        .T.rename("2ldrm")
    )

    if casename == "Australia":
        ds.coords["lon"] = (ds["lon"] + 180) % 360 - 180
        ds = ds.sortby(ds.lon)

    return ds


def read_flexpart_uib(basedir, casename):
    """Read data for flexpart uib."""
    print(f"Loading UIB data for {casename}")
    if casename in ["Scotland", "Australia"]:
        print(f"Skipping UIB data for {casename} - not available")
        return xr.Dataset()

    path = basedir / casename / "results UiB FLEXPART WaterSip"
    filename = f"{casename}_2022_UiB_Sodemann_grid_EN1_regridded.nc"
    return xr.open_dataset(path / filename)["moisture_uptakes"].rename("flexpart_uib")


def read_btrims(basedir, casename):
    """Read data for B-TrIMS."""
    if casename in ["Scotland", "Pakistan"]:
        print(f"Skipping data for btrims, {casename} - Unavailable")
        return xr.Dataset()
    print(f"Loading btrims data for {casename}")
    path = basedir / casename / "results_B-TrIMS"

    # TODO check this and refactor such that dates are not case dependent.
    for day in range(22, 29):
        ds = xr.open_dataset(path / f"bt.202202_{day:02d}_processed.nc")
        if day == 22:
            ods = ds.wvcont_mm_daily
        else:
            ods += ds.wvcont_mm_daily

    ds = xr.Dataset(
        data_vars={"wvcont_mm_daily": (["latitude", "longitude"], ods.values)},
        coords={
            "latitude": (["latitude"], ds.latitude[:, 0].values),
            "longitude": (["longitude"], ds.longitude[0, :].values),
        },
    )["wvcont_mm_daily"]
    ds.coords["longitude"] = (ds.coords["longitude"] + 180) % 360 - 180

    return (
        ds.sortby(ds.longitude)
        .rename(latitude="lat", longitude="lon")
        .rename("B-TrIMS")
    )


def read_data(basedir, casename):
    """Examples of data loading of moisture sources

    Data is loaded per individual ensemble member for each model.
    All data is converted to mm evaporative sources over the whole time period.
    """
    # TODO check completeness

    # Combine cumulative moisture sources for all models in one netcdf
    basedir = Path(basedir)

    return xr.merge(
        [
            read_2ldrm(basedir, casename),
            read_flexpart_uib(basedir, casename),
            read_lagranto_chc(basedir, casename),
            read_flexpart_xu(basedir, casename),
            read_flexpart_tatfancheng(basedir, casename),
            read_tracmass(basedir, casename),
            read_ughent(basedir, casename),
            read_utrack(basedir, casename),
            read_uvigo(basedir, casename),
            read_wam2layers(basedir, casename),
            read_btrims(basedir, casename),
        ]
    )
